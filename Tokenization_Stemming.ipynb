{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOawoefaUNdvGqMD0geyI6k",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhavanajamalpur/Basics_git/blob/main/Tokenization_Stemming.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Libraries\n",
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk import sent_tokenize, word_tokenize"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6OOeMUIlHAOY",
        "outputId": "12763d53-9647-44f0-85c8-1c9c291f7b4f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Hello everyone! Welcome to SR University. You are studying NLP; subject for B.Tech/ M.Tech\"\n",
        "print(\"EXAMPLE :\")\n",
        "print(text)\n",
        "print(\"\\n\")\n",
        "# Sentence Tokenizer\n",
        "print(\"SENTENCE TOKENIZE :\")\n",
        "print(sent_tokenize(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s5ZQClwqHHvx",
        "outputId": "8e8ba4f7-8e17-4fef-eead-d508c9a3f68e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EXAMPLE :\n",
            "Hello everyone! Welcome to SR University. You are studying NLP; subject for B.Tech/ M.Tech\n",
            "\n",
            "\n",
            "SENTENCE TOKENIZE :\n",
            "['Hello everyone!', 'Welcome to SR University.', 'You are studying NLP; subject for B.Tech/ M.Tech']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Word Tokenizer\n",
        "print(\"WORD TOKENIZE\")\n",
        "print(word_tokenize(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PJDCnDrCJICl",
        "outputId": "b1290dba-e74c-487c-92a6-a3b52e498093"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WORD TOKENIZE\n",
            "['Hello', 'everyone', '!', 'Welcome', 'to', 'SR', 'University', '.', 'You', 'are', 'studying', 'NLP', ';', 'subject', 'for', 'B.Tech/', 'M.Tech']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#UUsing Spacy Tokenization\n",
        "#First, import the spaCy library and the relevant tokenizer\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "# Next, define the text that you want to tokenize\n",
        "text = \"Good Morning Dear Students.Today we will discuss Natural language processing .And topic is Tokeization....\"\n",
        "# Use the nlp object to tokenize the text\n",
        "doc = nlp(text)\n",
        " #Use the Sentence attribute to access the individual Sentences in the document\n",
        "for sentence in doc.sents:\n",
        " print(sentence)\n",
        " #Use the token attribute to access the individual tokens in the document\n",
        "for token in doc:\n",
        "  print(token.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ra5sBcUOKttp",
        "outputId": "aec707f5-a8d1-474a-c483-8ec5024e3161"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Good Morning Dear Students.\n",
            "Today we will discuss Natural language processing .And\n",
            "topic is Tokeization....\n",
            "Good\n",
            "Morning\n",
            "Dear\n",
            "Students\n",
            ".\n",
            "Today\n",
            "we\n",
            "will\n",
            "discuss\n",
            "Natural\n",
            "language\n",
            "processing\n",
            ".And\n",
            "topic\n",
            "is\n",
            "Tokeization\n",
            "....\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#TextBlob is a Python library for working with textual data\n",
        "from textblob import TextBlob\n",
        "# Define a text string\n",
        "text = \"Here  we will be using TextBlob a python Library for working with textual data\"\n",
        "\n",
        "# Create a TextBlob object\n",
        "blob = TextBlob(text)\n",
        "print(blob.words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3wWImickS_jN",
        "outputId": "b75f9001-454e-4d36-a706-1226680e4094"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Here', 'we', 'will', 'be', 'using', 'TextBlob', 'a', 'python', 'Library', 'for', 'working', 'with', 'textual', 'data']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Stemming\n",
        "from nltk.stem import PorterStemmer\n",
        "e_words= [\"wait\", \"waiting\", \"waited\", \"waits\",\"eats\",\"eating\",\"meeting\",\"program\",\"programming\",\"programer\",\"programs\",\"programmed\"]\n",
        "ps =PorterStemmer()\n",
        "#print(\"{0:20}{1:20}\".format(\"--Word--\",\"--Stem--\"))\n",
        "for word in e_words:\n",
        " print(word,\"....|..\",ps.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fkqx3SvYdoaP",
        "outputId": "6fffa4ea-a7b9-4cfd-896b-f5f2805803b0"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "wait ....|.. wait\n",
            "waiting ....|.. wait\n",
            "waited ....|.. wait\n",
            "waits ....|.. wait\n",
            "eats ....|.. eat\n",
            "eating ....|.. eat\n",
            "meeting ....|.. meet\n",
            "program ....|.. program\n",
            "programming ....|.. program\n",
            "programer ....|.. program\n",
            "programs ....|.. program\n",
            "programmed ....|.. program\n"
          ]
        }
      ]
    }
  ]
}